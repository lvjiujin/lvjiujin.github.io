### GloVe: Global Vectors for Word Representation



## 一、总览



Glove, short for "Global Vectors for Word Representation", 顾名思义，从全局角度构建word2vec. 

本质做法：在MF（矩阵分解） + word2vec 的结合, 充分利用各自的优点，同时克服对应的缺点。

MF（PLSA, SVD等)： 优点：充分利用全局的统计信息，构建词贡献矩阵。

​									 缺点：在词类比任务上表现差。

Word2vec(skip-gram, cbow): 优点：利用局部的上下文窗口信息训练的词向量在词类比任务上表现不错。

​													缺点：没有充分利用全局的统计信息。（学习不到全局的信息）

## 二、重点关注点

Glove的数学模型，就是到底是怎么做的？

Glove与其他模型的比较。

## 三、论文详细剖析

## Abstract

Recent methods for learning vector space representations of words have succeeded in capturing fine grained semantic and syntactic regularities (细粒度的语法和语义规则) using vector arithmetic,but the origin of these regularities has remained opaque.(但是这些规则的源头仍然不清楚，不透明)

We analyze and make explicit the model properties needed for such regularities to emerge in word vectors.

我们分析并明确了在词向量中出现这些规则所需的模型属性。

<table><tr><td bgcolor=violet>The result is a new global log bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus.</td></tr></table>

结果就是一种新的全局对数双线性回归模型结合了两种重要的模型家族：全局矩阵分解和局部上下文窗口方法。

## Introduction

Most word vector methods rely on the distance or angle between pairs of word vectors as the primary
method for evaluating the intrinsic quality of such a set of word representations.

过去的评价词向量质量的方法： 大多数词向量方法依赖词向量对之间的角度或者距离作为主要评价一组词表示的内在质量的方法。

Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies (单词类比) that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. 

新的方法：利用词向量空间的更精细的结构，不再是词向量之间的数量距离，而是多个维度上的不同。

For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. 

这个经典句子的英文表达

This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). 

这种评估方案有利于产生意义维度的模型，从而捕获分布式表示的多聚类思想。

<table><tr><td bgcolor=violet>分布式表示本质上就是一种多聚类思想</td></tr></table>

<table><tr><td bgcolor=violet>The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwesteret al., 1990) and    

    2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). </td></tr></table>

Currently, both families suffer significant drawbacks. While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure.  

MF(LAS, SVD) 矩阵分解高效利用统计信息，但是相对的在词类比任务上表现很差。

Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts. 

SG在词对任务上表现好，但是不能很好的利用语料库的统计信息，因为他们在训练时只是运用孤立的局部上下文窗口，而不是全局的共现矩阵。

In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so. 

<table><tr><td bgcolor=violet>We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. </td></tr></table>

我们提出了一种特定权重最小二乘法的模型，用来训练全局的词与词之间的共现矩阵计数值。

The model produces a word vector space with meaningful substructure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset.



### Glove Model

